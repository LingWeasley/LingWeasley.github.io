<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.8.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.8.0" type="image/png" sizes="32x32"><meta name="description" content="小样本生物声学事件检测是一项检测给定几个例子的新声音的发生时间的任务。以前的方法采用度量学习，用不同声音类别的标记部分建立一个潜在的空间，也就是所谓的正面事件。在这项研究中，我们提出了一个段级的小样本学习框架，在模型优化过程中同时利用正面和负面事件。用负面事件进行训练，其数量比正面事件大，可以提高模型的泛化能力。此外，在训练过程中，我们在验证集上使用过渡性推理，以更好地适应新的类别。我们通过对输入">
<meta property="og:type" content="article">
<meta property="og:title" content="SEGMENT-LEVEL METRIC LEARNING FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION">
<meta property="og:url" content="http://example.com/2023/02/10/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/index.html">
<meta property="og:site_name" content="漏勺的加油站">
<meta property="og:description" content="小样本生物声学事件检测是一项检测给定几个例子的新声音的发生时间的任务。以前的方法采用度量学习，用不同声音类别的标记部分建立一个潜在的空间，也就是所谓的正面事件。在这项研究中，我们提出了一个段级的小样本学习框架，在模型优化过程中同时利用正面和负面事件。用负面事件进行训练，其数量比正面事件大，可以提高模型的泛化能力。此外，在训练过程中，我们在验证集上使用过渡性推理，以更好地适应新的类别。我们通过对输入">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-02-10T05:57:35.000Z">
<meta property="article:modified_time" content="2023-06-18T05:58:22.493Z">
<meta property="article:author" content="小勺">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary"><title>SEGMENT-LEVEL METRIC LEARNING FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION | 漏勺的加油站</title><link ref="canonical" href="http://example.com/2023/02/10/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.8.0"><link rel="stylesheet" href="css/custom.css"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 6.0.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">SEGMENT-LEVEL METRIC LEARNING FOR FEW-SHOT BIOACOUSTIC EVENT DETECTION</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2023-02-10</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2023-06-18</span></span></div></header><div class="post-body"><p>小样本生物声学事件检测是一项检测给定几个例子的新声音的发生时间的任务。以前的方法采用度量学习，用不同声音类别的标记部分建立一个潜在的空间，也就是所谓的正面事件。在这项研究中，我们提出了一个段级的小样本学习框架，在模型优化过程中同时利用正面和负面事件。用负面事件进行训练，其数量比正面事件大，可以提高模型的泛化能力。此外，在训练过程中，我们在验证集上使用过渡性推理，以更好地适应新的类别。我们通过对输入特征、训练数据和超参数的不同设置对我们提出的方法进行了消融研究。我们最终的系统在DCASE2022挑战任务5（DCASE2022-T5）验证集上取得了62.73的F值，比基线原型网络34.02的性能要好得多。使用建议的方法，我们提交的系统在DCASE2022-T5中排名第二。本文的代码是完全开放的1</p>
<p>小样本学习（FSL）[1]是一个机器学习问题，根据包含有限信息的训练数据进行预测。声音事件检测（SED）[2]是一项定位某些声音类别的开始和偏移的任务。通过结合FSL和SED[3]的思想，一个系统可以只用几个例子来检测一个新的声音类型。少量的SED对于音频数据的标注很有用，特别是当用户需要检测一种新的声音类型时。之前的研究大多使用原型网络[4]作为主要架构[5, 6, 7, 8]。Yang等人[5]提出了一个相互学习的框架，该框架采用过渡推理来迭代改进特征提取器和分类器，其中过渡推理意味着模型在训练过程中可以接触到没有标签的测试集。嵌入空间的平滑流形可以帮助扩展决策边界并减少数据表示中的噪声[9]。Tang等人[6]提出在小样本学习的SED中使用嵌入传播[9]，通过基于相似性图的模型输出特征之间的内插来学习一个更平滑的流形。在[7，8]中描述的方法中，使用了诸如spec-augment和mixup等数据增强。还有一种基于频谱图-交叉相关的方法，称为模板匹配[10]，它根据例子声音事件和未标记数据之间的归一化交叉相关进行检测。</p>
<p>度量学习[11]指的是为一项任务学习距离函数和特征空间。以前基于度量学习的研究[5, 6]通常用标记的正面事件来优化模型，通过对具有相同和不同类别的事件的潜在原型分别进行分组和分离。不包含目标事件的音频块，我们称之为负面事件，其数量较大，但受到的关注较少。例如，在DCASE 2022任务5开发集[10]中，负面事件的持续时间为19.18小时，占训练数据的91.3%，总时间为21小时。</p>
<p>在本文中，我们提出了一个分段级的度量学习方法，在小样本学习的生物声学检测任务上取得了最先进的结果。如图1所示，我们的系统在段级上运行。每个声音事件可以包含多个片段。我们训练一个特征提取网络，将片段映射到潜伏嵌入中，这些嵌入被平均到原型中以代表不同的声音类别。为了学习一个稳健的潜伏空间，我们使用了一个过渡性的学习方案，并提议用负面事件建立对比性损失。我们还通过使用特征选择、数据增强和后处理来改进我们的方法。我们进行了消融研究以衡量每个组成部分的有效性。我们提出的方法在DCASE任务5验证集上取得了62.73的F值。本文将组织如下。第2节提供了我们系统的概述。第3节介绍了我们的方法。第4节讨论了实验设置。第5节报告评估结果和消融研究。第6节总结了这项工作并提供了一个结论。</p>

        <h1 id="系统概述"   >
          <a href="#系统概述" class="heading-link"><i class="fas fa-link"></i></a><a href="#系统概述" class="headerlink" title="系统概述"></a>系统概述</h1>
      <p>我们使用一个原型网络[4]建立我们的系统，该网络被广泛用于基于度量的小样本学习。训练数据T &#x3D; (Si, Yi)|Ntrain i&#x3D;1包含音频特征集Si &#x3D; {si|yi &#x3D; 1} t { ∼ si|yi &#x3D; 0}及其相应的标签集Yi &#x3D; {yi|yi∈ {0, 1}}，其中{si}和{ ∼ si}分别是i类的正片和负片的集合，Ntrain是训练片段的总数目。评估数据集E &#x3D; (S′ i, Y ′ i )|Neval i&#x3D;1还包含一个音频特征集S′ i &#x3D; {s′ i}和一个标签集Y′ i &#x3D; {y′ i}，其中Neval是评估集中的类的数量，|S′ i| &#x3D; Li和|Y ′ i| &#x3D; K。这里我们有Li≥K，因为评价集只有前K个事件的部分标签。我们系统的目标是将不同的音频特征正确地映射到高维空间的潜伏嵌入中，其中类似的音频特征比较接近。</p>
<p>我们使用轮次训练[12]，以N-way-M-shot的方式优化我们的系统。如图2(a)所示，N-way-Mshot意味着每个训练批次将选择N个类的数据。对于每个类i，系统将随机选择M个片段{ss ij }j&#x3D;1…M作为支持片段，另外M个片段{sq ij }j&#x3D;1…M作为查询片段。所有不同类别的片段都有相同的长度。然后，一个特征提取网络（第3.1节）将这些片段映射成固定长度的嵌入，然后将其平均化为查询原型xq i和支持原型xs i。该系统通过最小化查询和支持相同类别的原型之间的距离进行优化。为了建立一个稳健的潜在空间并更好地泛化到新的类别，我们建议在第3.2节和3.3节中使用度量学习中的负面事件和过渡性推理方案。</p>
<p>在评估过程中，音频文件将使用一个具有自适应段长的滑动窗口进行分割（第3.4节）。标记部分的片段将被用来建立正面和负面的原型，这些原型被视为音频文件中正面和负面事件的潜在代表。未标记部分的片段是查询集，可以通过计算和比较与正、负原型的距离来进行分类（第3.5节）。而如果一个查询属于正向原型的概率大于阈值h，它将被归为正向。连续的正面预测将被合并为一个单一的事件。</p>

        <h1 id="方法论"   >
          <a href="#方法论" class="heading-link"><i class="fas fa-link"></i></a><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1>
      <p>特征提取网络 我们的特征提取网络fθ是一个基于卷积神经网络（CNN）的架构，它将音频特征s映射到潜伏嵌入x中。与[13]提出的架构类似，网络fθ由三个卷积块组成，隐藏通道大小为64、128和64。每个卷积块由三个二维CNN层组成，具有批量归一化和漏整流线性单元激活[14]。作为基于CNN的网络中常见的技巧[13, 15]，我们在每个块之后应用2×2的最大池，用于下采样和扩大接收场。每个卷积块的输入和输出都有一个由下采样CNN层处理的剩余连接。为了在不同的输入长度下保持相同的输出维度，我们在网络的末端应用自适应平均池化。自适应池化后的最终输出特征图是一个C×T×F大小的块，这就是s的最终潜伏嵌入。</p>

        <h1 id="“Segment-level-metric-learning”-Liu-等-2022-p-2-pdf"   >
          <a href="#“Segment-level-metric-learning”-Liu-等-2022-p-2-pdf" class="heading-link"><i class="fas fa-link"></i></a><a href="#“Segment-level-metric-learning”-Liu-等-2022-p-2-pdf" class="headerlink" title="“Segment-level metric learning” (Liu 等, 2022, p. 2) (pdf)"></a>“Segment-level metric learning” (<span class="exturl"><a class="exturl__link"   href="zotero://select/library/items/BYLIN3NY" >Liu 等, 2022, p. 2</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>) (<span class="exturl"><a class="exturl__link"   href="zotero://open-pdf/library/items/GMG38YGH?page=2" >pdf</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>)</h1>
      <p>我们建议在模型优化过程中利用负面事件中的负面片段来学习更稳健的表示，如图2（b）所示。与[3]类似，我们首先将音频特征划分为长度相等的片段，用于度量学习。然后fθ将所有的片段映射成潜伏嵌入。在优化过程中，我们将计算查询原型xq i的类概率分布，这涉及到与所有正面和负面支持原型的距离计算。在这种情况下，模型可以从负面事件中学习到更多的关于建立潜在空间的对比性信息。具体来说，我们首先根据公式1计算出一个距离矩阵D&#x3D;[d(1), d(2), …, d(N)]T。</p>
<p>其中d(i)∈R2N代表xq i与2N个支持原型之间的距离，～ xs j表示j类的负面事件的支持原型。然后，我们通过最大化xq i接近第i类的正支持原型xs i的概率来优化我们的模型，给出的结果是</p>
<p>其中0≤i，j≤N，i，j∈N，l是目标函数。注意，学习过程不涉及负面事件～xq i的查询原型，因为～xq i和～xs i不能保证有相同的声音类型。</p>
<p>数据平衡在这个任务中很重要，因为不同的声音类别有不同的总时长[10]。为了平衡不同的类别，我们在轮次训练中对每个类别进行等概率的采样。这样一来，模型对每个类别的关注概率相等，就不容易出现过拟合的情况。</p>

        <h1 id="“Transductive-inference”-Liu-等-2022-p-2-pdf"   >
          <a href="#“Transductive-inference”-Liu-等-2022-p-2-pdf" class="heading-link"><i class="fas fa-link"></i></a><a href="#“Transductive-inference”-Liu-等-2022-p-2-pdf" class="headerlink" title="“Transductive inference” (Liu 等, 2022, p. 2) (pdf)"></a>“Transductive inference” (<span class="exturl"><a class="exturl__link"   href="zotero://select/library/items/BYLIN3NY" >Liu 等, 2022, p. 2</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>) (<span class="exturl"><a class="exturl__link"   href="zotero://open-pdf/library/items/GMG38YGH?page=2" >pdf</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>)</h1>
      <p>在训练过程中，我们采用了过渡性推理[17]的方法，这意味着我们的模型将在完全标记的训练集和部分标记的评估数据上进行优化。评估数据中的每个文件都有第一个K标记的特定类型的声音的事件。我们把这K个事件作为正面事件，而把标记部分的其余K个音频块作为负面事件。在评价集中，虽然每个文件的声音类型不可用，但具有相同声音类型的文件应该在同一个子文件夹中，我们把评价集中的每个子文件夹视为不同的声音类型。尽管每个子文件夹中的文件不一定都包含相同的目标声音，但我们的实验表明，通过这种方式进行归纳推理，仍然可以帮助模型获得对评价集更好的适应（第3节）。</p>

        <h1 id="“Adaptive-segment-length”-Liu-等-2022-p-3-pdf"   >
          <a href="#“Adaptive-segment-length”-Liu-等-2022-p-3-pdf" class="heading-link"><i class="fas fa-link"></i></a><a href="#“Adaptive-segment-length”-Liu-等-2022-p-3-pdf" class="headerlink" title="“Adaptive segment length” (Liu 等, 2022, p. 3) (pdf)"></a>“Adaptive segment length” (<span class="exturl"><a class="exturl__link"   href="zotero://select/library/items/BYLIN3NY" >Liu 等, 2022, p. 3</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>) (<span class="exturl"><a class="exturl__link"   href="zotero://open-pdf/library/items/GMG38YGH?page=3" >pdf</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>)</h1>
      <p>在训练期间，为了方便批量处理，我们在所有类别中使用相同的段长。但是在评估过程中，使用相同的片段长度并不理想。例如，使用太长或太短的片段长度将分别倾向于有一个高的假阴性率或假阳性率。在评估集中，不同的动物或鸟类的发声长度有很大的不同，从30毫秒到5秒不等。因此，我们选择在评估过程中使用自适应的片段长度。</p>
<p>如表1所示，我们根据标记事件的最大长度tmax &#x3D; max(t1, …, tK )，为每个音频文件设置不同的片段长度，其中t1, …, tK表示K个标记的正面事件的持续时间。我们将跳跃长度设定为窗口长度的三分之一。注意这里的参数是根据经验选择的。</p>

        <h1 id="“Positive-and-negative-prototypes”-Liu-等-2022-p-3-pdf"   >
          <a href="#“Positive-and-negative-prototypes”-Liu-等-2022-p-3-pdf" class="heading-link"><i class="fas fa-link"></i></a><a href="#“Positive-and-negative-prototypes”-Liu-等-2022-p-3-pdf" class="headerlink" title="“Positive and negative prototypes” (Liu 等, 2022, p. 3) (pdf)"></a>“Positive and negative prototypes” (<span class="exturl"><a class="exturl__link"   href="zotero://select/library/items/BYLIN3NY" >Liu 等, 2022, p. 3</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>) (<span class="exturl"><a class="exturl__link"   href="zotero://open-pdf/library/items/GMG38YGH?page=3" >pdf</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>)</h1>
      <p>在评估过程中，我们假设前K个标记的正面事件不包含太多种类，因此我们通过平均标记的正面片段的嵌入来计算正面原型。相比之下，建立负面原型就比较麻烦了，因为负面片段可能包含许多不同种类的声音。因此，简单地对所有的负面嵌入进行平均，会导致负面原型的次优表现。为了解决这些挑战，我们选择运行我们的评估六次，每次选择30个随机选择的负片断，在标记的负片断中，我们平均六次运行的预测概率作为最终预测。每次运行中的负片原型可以有机会代表不同的声音。这个过程类似于随机子空间方法[18]，用不同的训练数据子集训练的几个估计器的集合可以胜过在全部训练数据上优化的单一估计器。</p>

        <h1 id="实验"   >
          <a href="#实验" class="heading-link"><i class="fas fa-link"></i></a><a href="#实验" class="headerlink" title="实验"></a>实验</h1>
      <p>DCASE2022-T5 DCASE 2022任务5数据集2包含一个训练集、一个验证集和一个官方评估集。训练集和验证集都是全标签的。官方评估集有前五个正面事件的标签。在撰写本文时，官方评价集的完整标签还没有公布，因此，在本文中，我们按照[19, 20]将验证集视为评价集。训练期间的验证并不是为了挑选最佳模型。这是因为我们以不同于评估的方式进行验证。与训练过程类似，我们在固定长度的片段水平上计算验证的准确性，没有自适应的片段长度。因此，验证中的最佳模型不一定在评估中表现最好。尽管如此，我们在实验中使用了相同的验证过程，因此在不同的环境下进行的比较是公平的。在[21，22]中也有类似的想法，它们利用评估集进行验证。</p>
<p>AudioSet-Aminal-SL AudioSet[23]是一个用于音频研究的大规模数据集[13, 24]。考虑到DCASE2022-T5的训练集只包含47个不同的声音类别，我们选择使用AudioSet数据集3的强标签部分，以增加训练数据的种类。为了缓解领域不匹配的问题，我们只使用与动物发声有关的声音标签，并且不与其他非动物声音重叠。经过数据清理，我们从AudioSet中得到了1796个具有37个类别的音频。然而，即使声音在AudioSet中具有相同的标签，它们的声音仍然可能非常不同。为了缓解这个问题，我们把AudioSet中的每个音频文件都当作自己的类，所以我们在这个数据集中有1796个类，它被命名为AudioSet-Aminal-SL，其中SL表示强标签。为了平衡AudioSet-Aminal-SL和DCASE2022-T5中的1796个类和47个类，我们在轮次训练中从每个数据集中选择一半的类。</p>
<p>我们使用DCASE任务5的组织者提供的官方评价指标F-measure分数作为我们的主要评价指标。我们还用多声部声音检测得分（PSDS）[25]报告系统性能，这是一个基于交集的声音事件检测的稳健评价指标。在PSDS中，我们将检测容忍度标准（DTC）和地面真实相交标准（GTC）设定为0.5，最大有效误判率为100.0。其他参数如交叉触发容限准则（CTTC）没有被使用，因为我们的任务不是多声道检测。</p>
<p>按照[5]，所有的音频数据都被重新取样为22.5kHz的采样率。我们系统的输入特征是PCEN[26]和∆MFCC[27]特征的堆叠。在短时傅里叶变换中，我们设定窗长为1024，跳数为256。我们将melf frequency维度设置为128。在训练过程中，我们模型的输入长度为0.2秒。如果声音事件小于0.2秒，将应用零填充。第3.1节中提到的嵌入的大小为2048，其中C&#x3D;64，T&#x3D;4，F&#x3D;8。所有的实验都使用0.001的初始学习率，每10个历时有0.65的指数衰减。我们在每个历时后进行验证。我们以3-5-shot的方式进行验证，因为在验证集中只有三个类（HB、ME、PB）。如果连续10个历时的验证精度没有提高，我们将停止模型训练。而具有最佳验证准确率的模型被用于评估。为了充分利用训练数据，我们实现了一个动态数据加载器，该加载器在飞行中生成具有随机起始时间的训练数据。我们假设某种动物的一次发声时间不会有很大的变化。因此，我们根据阳性事件的最大长度 tmax &#x3D; max(t1, …, tK )来设计一个声音类别的后期处理策略。如果一个阳性检测的长度小于α ∗ tmax或大于β ∗ tmax，我们将删除它。在评估过程中，我们使用β&#x3D;2.0，α&#x3D;[0.1, 0.2, …, 0.9]，以及阈值h&#x3D;[0.0, 0.05, …, 0.95]。我们使用β、α、h的不同组合来计算数据点[25]，绘制PSD-ROC曲线，并计算PSDS。我们在所有β, α, h的组合中选择最佳的F-measure作为最终的F-measure。</p>

        <h1 id="结果"   >
          <a href="#结果" class="heading-link"><i class="fas fa-link"></i></a><a href="#结果" class="headerlink" title="结果"></a>结果</h1>
      <p>我们的系统在评估集上的表现报告在表2中。模板匹配和我们重新实现的原型网络基线[10]的F-measure得分分别为4.28和34.02。我们的系统在很大程度上超过了基线，F-measure分数为62.73，PSDS为57.52。</p>
<p>如图3所示，使用转导推理可以显著提高验证的准确性。而从类的ROC来看，HB类，主要是蚊子的声音，是最容易检测的一类。PB类是最难的一类，也许是因为它主要由稀疏的鸟叫声和强背景噪声组成。ME类在评估集中取得了平均性能。</p>
<p>我们对输入特征的影响进行了研究。如图4(a)所示，F-measure和PSDS的性能并不总是一致的，考虑到F-measure在先前的研究中被广泛使用[10]，我们使用F-measure来指导我们的选择。通过比较Fmeasure得分，PCEN+∆MFCC在评估集上似乎是一个好的特征组合。我们还在图4（b）中比较了不同的嵌入维度。我们通过改变自适应平均池中F的维度来改变这个维度。我们注意到512的维度比256的维度有很大的改善，而2048的维度在所有设置中表现最好。</p>
<p>我们对我们提出的每个组件进行消融。如表3所示，如果我们删除负面的片段，性能就会大大下降。这一趋势与过渡性推理和后处理是一样的。我们还研究了训练数据的影响。在表4中，我们可以看到，仅使用DCASE2022-T5就能获得最好的F-measure得分。使用AudioSet-SL会导致46.83的F-measure和51.00的PSDS。通过结合两个数据集，我们得到了58.48的F-measure和58.77的最佳PSDS。我们假设，使用AudioSet的F-measure的下降是由训练数据的领域不匹配造成的。然而，结合两个数据集产生了最好的PSDS，这意味着使用AudioSet数据可以导致所有阈值和后处理设置的普遍改善，而不是得到一个具有高F-measure的单一最佳系统。这表明PSDS可能是一个合适的指标，供社区在这项任务中参考。</p>

        <h1 id="结论"   >
          <a href="#结论" class="heading-link"><i class="fas fa-link"></i></a><a href="#结论" class="headerlink" title="结论"></a>结论</h1>
      <p>本文提出了一个新的框架，用于少见的声音事件检测。我们提出的带负数段的度量学习和归纳推理方案可以显著提高模型性能。在输入特征上，我们的实验表明，在我们的设置中，带有∆MFCC的PCEN产生了最好的性能。我们的结果还表明，通过在评估过程中考虑多个阈值和后处理设置，PSDS可能是评估模型整体性能的一个有用指标</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://example.com">小勺</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://example.com/2023/02/10/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/">http://example.com/2023/02/10/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://example.com/tags/%E8%AE%BA%E6%96%87/">论文</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2023/02/15/Prototypical%20Networks%20for%20Few-shot%20Learning/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">Prototypical Networks for Few-shot Learning</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2023/01/01/SE/"><span class="paginator-prev__text">Squeeze-and-Excitation Networks-</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">
          系统概述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E8%AE%BA"><span class="toc-number">2.</span> <span class="toc-text">
          方法论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E2%80%9CSegment-level-metric-learning%E2%80%9D-Liu-%E7%AD%89-2022-p-2-pdf"><span class="toc-number">3.</span> <span class="toc-text">
          “Segment-level metric learning” (Liu 等, 2022, p. 2) (pdf)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E2%80%9CTransductive-inference%E2%80%9D-Liu-%E7%AD%89-2022-p-2-pdf"><span class="toc-number">4.</span> <span class="toc-text">
          “Transductive inference” (Liu 等, 2022, p. 2) (pdf)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E2%80%9CAdaptive-segment-length%E2%80%9D-Liu-%E7%AD%89-2022-p-3-pdf"><span class="toc-number">5.</span> <span class="toc-text">
          “Adaptive segment length” (Liu 等, 2022, p. 3) (pdf)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E2%80%9CPositive-and-negative-prototypes%E2%80%9D-Liu-%E7%AD%89-2022-p-3-pdf"><span class="toc-number">6.</span> <span class="toc-text">
          “Positive and negative prototypes” (Liu 等, 2022, p. 3) (pdf)</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">7.</span> <span class="toc-text">
          实验</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number">8.</span> <span class="toc-text">
          结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">9.</span> <span class="toc-text">
          结论</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/tx.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">漏勺的加油站</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">23</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">10</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">6</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2023</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>小勺</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.8.0"></script><script src="/js/stun-boot.js?v=2.8.0"></script><script src="/js/scroll.js?v=2.8.0"></script><script src="/js/header.js?v=2.8.0"></script><script src="/js/sidebar.js?v=2.8.0"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"live2d-widget-model-miku"},"display":{"position":"left","width":150,"height":190,"hOffset":50,"vOffset":-5},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.8},"log":false});</script></body></html>